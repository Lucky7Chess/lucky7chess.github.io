
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Modified from: Jon Barron and Saurabh Gupta. Thanks to Jeff Donahue for email scamble.*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  span.highlight {
  background-color: #ffffd0;
  }
  </style>
  <link rel="shortcut icon" href="https://www.cs.cmu.edu/sites/default/files/favicon_0.ico" type="image/vnd.microsoft.icon">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Kenneth Shaw</title>
  <meta name="Kenneth" http-equiv="Content-Type" content="Kenneth Shaw">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-64069893-1', 'auto');
    ga('send', 'pageview');
  </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>

<body>
<table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <!-- <font size="7">Russell Mendonca</font><br> -->
    <pageheading>Kenneth Shaw</pageheading><br>
    <!-- <b>email</b>:&nbsp
    <font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font>
    <script>
    emailScramble = new scrambledString(document.getElementById('email'),
        'emailScramble', 'umepc@aah.tud.skdc',
        [18,13,16,2,12,8,6,3,5,15,4,14,17,11,10,7,1,9]);
    </script> -->
  </p>

  <tr>
    <td width="32%" valign="top"><img src="images/profile.jfif" width="100%" style="border-radius:15px"></a>
    <p align=center>
    <a href="./files/cv.pdf">CV</a> |
    <a href="https://scholar.google.com/citations?user=T3yF9yWrz2UC&hl=en&oi=sra">Google Scholar</a>  |
    <!-- <a href="https://github.com/russellmendonca">Github</a> | --><br>
    <a href="https://twitter.com/kenny__shaw">Twitter</a> | 
    <a href="./files/talk.txt">Formal Bio</a> | 
    <br/>
    </p>
    </td>
    <td width="68%" valign="top" align="justify">
    <p> Hi I'm Kenny.  I'm a 3rd year PhD student (as of fall 2025) at the Robotics Institute in Carnegie Mellon University,
        advised by <a href="https://www.cs.cmu.edu/~dpathak/"> Prof. Deepak Pathak </a>.
        <br>
        <br>
        <!-- Think about typing on a keyboard, using chopsticks, or hammering a nail—our hands allow us to interact with the world with remarkable precision and adaptability. In contrast, robotic manipulation is still largely limited to basic grippers. A key reason is that truly dexterous robot hands are notoriously difficult to build and control. -->
        <!-- <br><br> -->
        My reserach centers on dexterous manipulation.  I have designed several low-cost, highly capable dexterous robotic hands aimed at making manipulation research and education more accessible. I develop highly dexterous control policies for these hands by leveraging human demonstrations from internet videos, teleoperation, and simulation.
        <br><br>
        More broadly, I'm interested in how can we create new democratized robotic hardware systems with unlocked capabillities from machine learning. How does the design of a robot's hardware shape the way it learns—and how does the learning influence how the hardware should be designed?
        <br>
        <p> Previously, I graduated from Georgia Tech in Computer Engineering and worked on multi-agent systems and HRI with Prof. Sonia Chernova and Prof. Harish Ravichandar.
        Please contact me via email at kshaw2 -at- andrew dot cmu dot edu .  
    </p>
    
    </td>
  </tr>
</table>

<hr/>




<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Recent Publications</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
<br/>



<tr>
  <tr>
    <td width="33%" valign="top" align="center"><a href="https://dexwild.github.io/">
      <video autoplay loop muted src="./images/dexwild_2.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black"></video>
    </a></td>
    <td width="67%" valign="top">
      <p><a href="https://dexwild.github.io/">
      <heading> DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies </heading>
      </a><br>
      Tony Tao*, Mohan Kumar Srirama*, Jason Jingzhou Liu, Kenneth Shaw, Deepak Pathak<br>
      RSS 2025
      </p>
      <div class="paper" id="dexwild_tog">
        <a href="https://dexwild.github.io/">website</a>
        |
        <a href="javascript:toggleblock('dexwild_abs')">abstract</a>
        |
        <a shape="rect" href="javascript:togglebib('dexwild_tog')" class="togglebib">bibtex</a>
        |
        <a href="https://arxiv.org/abs/2505.07813">arXiv</a>

        <p align="justify">
          <i style="display: none;" id="dexwild_abs">
            Large-scale, diverse robot datasets have emerged as a promising path toward enabling dexterous
            manipulation policies
            to generalize to novel environments, but acquiring such datasets presents many challenges. While
            teleoperation provides
            high-fidelity datasets, its high cost limits its scalability. Instead, what if people could use
            their own hands, just
            as they do in everyday life, to collect data? In DexWild, a diverse team of data collectors uses
            their hands to collect
            hours of interactions across a multitude of environments and objects. To record this data, we
            create DexWild-System,
            a low-cost, mobile, and easy-to-use device. The DexWild learning framework co-trains on both
            human and robot
            demonstrations, leading to improved performance compared to training on each dataset
            individually. This combination
            results in robust robot policies capable of generalizing to novel environments, tasks, and
            embodiments with minimal
            additional robot-specific data. Experimental results demonstrate that DexWild significantly
            improves performance,
            achieving a 68.5% success rate in unseen environments-nearly four times higher than policies
            trained with robot data
            only-and offering 5.8x better cross-embodiment generalization.
          </i>
        </p>

        <pre xml:space="preserve" style="display:none;">@article{tao2025dexwild,
title={DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies},
author={Tao, Tony and Srirama, Mohan Kumar and Liu, Jason Jingzhou and Shaw, Kenneth and Pathak, Deepak},
journal={Robotics: Science and Systems (RSS)},
year={2025},
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://jasonjzliu.com/factr/">
      <video autoplay loop muted src="./images/factr.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black"></video>
    </a></td>
    <td width="67%" valign="top">
      <p><a href="https://jasonjzliu.com/factr/"><heading>
        FACTR: Force-Attending Curriculum Training
          for Contact-Rich Policy Learning</heading></a>  <br>
      Jason Jingzhou Liu*, Yulong Li*, Kenneth Shaw, Tony Tao, Ruslan Salakhutdinov, Deepak Pathak<br>
      RSS 2025 
      </p> 
      <div class="paper" id="factr_tog">
        <a href="https://jasonjzliu.com/factr/">website</a>
        |
        <a href="javascript:toggleblock('factr_abs')">abstract</a>
        |
        <a shape="rect" href="javascript:togglebib('factr_tog')" class="togglebib">bibtex</a>
        |
        <a href="https://arxiv.org/abs/2502.17432">arXiv</a>
        |
        <a href="https://github.com/RaindragonD/factr/">code</a>

        <p align="justify">
          <i style="display: none;" id="factr_abs">
            Many contact-rich tasks humans perform, such as box pickup or rolling dough, rely on
            force feedback for reliable execution. However, this force information, which is readily
            available in most robot arms, is not commonly used in teleoperation and policy learning.
            Consequently, robot behavior is often limited to quasi-static kinematic tasks that do not
            require intricate force-feedback. In this paper, we first present a low-cost, intuitive,
            bilateral teleoperation setup that relays external forces of the follower arm back to
            the teacher arm, facilitating data collection for complex, contact-rich tasks. We
            then introduce FACTR, a policy learning method that employs a curriculum which corrupts
            the visual input with decreasing intensity throughout training. The curriculum prevents
            our transformer-based policy from over-fitting to the visual input and guides the policy
            to properly attend to the force modality. We demonstrate that by fully utilizing the
            force information, our method significantly improves generalization to unseen objects
            by 43% compared to baseline approaches without a curriculum.
          </i>
        </p>

        <pre xml:space="preserve" style="display:none;">@article{liu2025factr,
        title={FACTR: Force-Attending Curriculum Training for 
        Contact-Rich Policy Learning}, 
        author={Jason Jingzhou Liu and Yulong Li and Kenneth Shaw 
        and Tony Tao and Ruslan Salakhutdinov and Deepak Pathak},
        journal={arXiv preprint arXiv:2502.17432},
        year={2025},
        }
        </pre>
      </div>
      <p></p>
      <p></p>
    </td>
  </tr>


  <tr bgcolor="#ffffd0">
    <td width="33%" valign="top" align="center"><a href="https://bidex-teleop.github.io/">
    <video autoplay loop muted src="https://bidex-teleop.github.io/resources/1/plate2.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black"></video>
    </a></td>
    <td width="67%" valign="top">
      <p><a href="https://bidex-teleop.github.io/" id="Bidex">
      <heading>Bimanual Dexterity for Complex Tasks</heading></a><br>
      Kenneth Shaw*, Yulong Li*, Jiahui Yang, Mohan Kumar Srirama, Ray Liu, Haoyu Xiong, Russell Mendonca†, Deepak Pathak†<br>
      CoRL 2024
      </p>
      <div class="paper" id="bidex">
      <a href="https://bidex-teleop.github.io/">webpage</a> |
      <a href="javascript:toggleblock('bidex_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('bidex')" class="togglebib">bibtex</a> |
      <a href="https://openreview.net/pdf?id=55tYfHvanf">CoRL</a>
  
      <p align="justify"> <i style="display: none;" id="bidex_abs">To train generalist robot policies, machine learning methods often require a substantial amount of expert human teleoperation data. An ideal robot for humans collecting data is one that closely mimics them: bimanual arms and dexterous hands. However, creating such a bimanual teleoperation system with over 50 DoF is a significant challenge. To address this, we introduce Bidex, an extremely dexterous, low-cost, low-latency and portable bimanual dexterous teleoperation system which relies on motion capture gloves and teacher arms. We compare Bidex to a Vision Pro teleoperation system and a SteamVR system and find Bidex to produce better quality data for more complex tasks at a faster rate. Additionally, we show Bidex operating a mobile bimanual robot for in the wild tasks. The robot hands (5k USD) and teleoperation system (7k USD) is readily reproducible and can be used on many robot arms including two xArms ($16k USD).</i></p>
  
      <pre xml:space="preserve" style="display:none;">
        @inproceedings{shaw2024bimanual,
          title={Bimanual Dexterity for Complex Tasks},
          author={Shaw, Kenneth and Li, Yulong and Yang, Jiahui and Srirama, Mohan Kumar and Liu, Ray and Xiong, Haoyu and Mendonca, Russell and Pathak, Deepak},
          booktitle={8th Annual Conference on Robot Learning},
          year={2024}
        }
      </pre>
      </div>
    </td>
</tr>


<tr>
  <tr>
    <td width="33%" valign="top" align="center"><a href="https://open-world-mobilemanip.github.io/">
    <img src="images/door_gif.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
    </a></td>
    <td width="67%" valign="top">
      <p><a href="https://open-world-mobilemanip.github.io/" id="DOOR">
      <heading>Adaptive Mobile Manipulation for Articulated Objects In the Open World</heading></a><br>
      Haoyu Xiong, Russell Mendonca, Kenneth Shaw, Deepak Pathak<br>
      ArXiv 2024
      </p>
  
      <div class="paper" id="door">
      <a href="https://open-world-mobilemanip.github.io/">webpage</a> |
      <a href="javascript:toggleblock('door_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('door')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2401.14403">arXiv</a>
  
      <p align="justify"> <i style="display: none;" id="door_abs">Deploying robots in open-ended unstructured environments such as homes has been a long-standing research problem. However, robots are often studied only in closed-off lab settings, and prior mobile manipulation work is restricted to pick-move-place, which is arguably just the tip of the iceberg in this area. In this paper, we introduce Open-World Mobile Manipulation System, a full-stack approach to tackle realistic articulated object operation, e.g. real-world doors, cabinets, drawers, and refrigerators in open-ended unstructured environments. We propose an adaptive learning framework in which the robot initially learns from a small set of data through behavior cloning, followed by learning from online self-practice on novel variations that fall outside the BC training domain. We develop a low-cost mobile manipulation hardware platform capable of repeatedly safe and autonomous online adaptation in unstructured environments with a cost of around 20k USD. We conducted a field test on 20 novel doors across 4 different buildings on a university campus. In a trial period of less than one hour, our system demonstrated significant improvement, boosting the success rate from 50% of BC pre-training to 95% of online adaptation without any human intervention.</i></p>
  
  <pre xml:space="preserve" style="display:none;">
    @article{xiong2024adaptive,
      title={Adaptive Mobile Manipulation for Articulated Objects In the Open World},
      author={Xiong, Haoyu and Mendonca, Russell and Shaw, Kenneth and Pathak, Deepak},
      journal={arXiv preprint arXiv:2401.14403},
      year={2024}
    }
  </pre>
      </div>
    </td>
</tr>

<tr>
  <tr>
    <td width="33%" valign="top" align="center"><a href="https://spin-robot.github.io/">
      <img src="images/spin.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
    </a></td>
    <td width="67%" valign="top">
      <p><a href="https://spin-robot.github.io/" id="SPIN">
      <heading>SPIN: Simultaneous Perception Interaction and Navigation</heading></a><br>
      Shagun Uppal, Ananye Agarwal, Haoyu Xiong, Kenneth Shaw, Deepak Pathak<br>
      CVPR 2024
      </p>
  
      <div class="paper" id="spin">
      <a href="https://spin-robot.github.io/">webpage</a> |
      <a href="javascript:toggleblock('spin_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('spin')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2405.07991">arXiv</a>
  
      <p align="justify"> <i style="display: none;" id="spin_abs">While there has been remarkable progress recently in the fields of manipulation and locomotion, mobile manipulation remains a long-standing challenge. Compared to locomotion or static manipulation, a mobile system must make a diverse range of long-horizon tasks feasible in unstructured and dynamic environments. While the applications are broad and interesting, there are a plethora of challenges in developing these systems such as coordination between the base and arm, reliance on onboard perception for perceiving and interacting with the environment and most importantly, simultaneously integrating all these parts together. Prior works approach the problem using disentangled modular skills for mobility and manipulation that are trivially tied together. This causes several limitations such as compounding errors, delays in decision-making and no whole-body coordination. In this work, we present a reactive mobile manipulation framework that uses an active visual system to consciously perceive and react to its environment. Similar to how humans leverage whole-body and hand-eye coordination, we develop a mobile manipulator that exploits its ability to move and see, more specifically -- to move in order to see and to see in order to move. This allows it to not only move around and interact with its environment but also, choose when to perceive what using an active visual system. We observe that such an agent learns to navigate around complex cluttered scenarios while displaying agile whole-body coordination using only ego-vision without needing to create environment maps.</i></p>
  
  <pre xml:space="preserve" style="display:none;">
    @InProceedings{Uppal_2024_CVPR,
      author    = {Uppal, Shagun and Agarwal, Ananye and Xiong, Haoyu and Shaw, Kenneth and Pathak, Deepak},
      title     = {SPIN: Simultaneous Perception Interaction and Navigation},
      booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
      month     = {June},
      year      = {2024},
      pages     = {18133-18142}
  }
  </pre>
      </div>
    </td>
</tr>



<tr>
  <tr>
    <td width="33%" valign="top" align="center"><a href="https://dexterous-finetuning.github.io/">
    <video autoplay loop muted src="https://dexterous-finetuning.github.io/static/videos/pour_cup.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black"></video>
    </a></td>
    <td width="67%" valign="top">
      <p><a href="https://dexterous-finetuning.github.io/" id="DEFT">
      <heading>DEFT: Dexterous Fine-Tuning for Real-World Hand Policies</heading></a><br>
      Aditya Kannan*, Kenneth Shaw*, Shikhar Bahl, Pragna Mannam, Deepak Pathak<br>
      CoRL 2023
      </p>
  
      <div class="paper" id="deft">
      <a href="https://dexterous-finetuning.github.io/">webpage</a> |
      <a href="javascript:toggleblock('deft_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('deft')" class="togglebib">bibtex</a> |
      <a href="https://openreview.net/pdf?id=wH23nZpVTF6">CoRL</a>
  
      <p align="justify"> <i style="display: none;" id="deft_abs">Dexterity is often seen as a cornerstone of complex manipulation. Humans are able to perform a host of skills with their hands, from making food to operating tools. In this paper, we investigate these challenges, especially in the case of soft, deformable objects as well as complex, relatively long-horizon tasks. Although, learning such behaviors from scratch can be data inefficient. To circumvent this, we propose a novel approach, DEFT (DExterous Fine-Tuning for Hand Policies), that leverages human-driven priors, which are executed directly in the real world. In order to improve upon these priors, DEFT involves an efficient online optimization procedure. With the integration of human-based learning and online fine-tuning, coupled with a soft robotic hand, DEFT demonstrates success across various tasks, establishing a robust, data-efficient pathway toward general dexterous manipulation.</i></p>
  
  <pre xml:space="preserve" style="display:none;">
  @article{kannan2023deft,
  title={DEFT: Dexterous Fine-Tuning for Real-World Hand Policies},
  author={Kannan, Aditya* and Shaw, Kenneth* and Bahl, Shikhar and Mannam, Pragna and Pathak, Deepak},
  journal= {CoRL},
  year={2023}
  }
  </pre>
      </div>
    </td>
</tr>

<tr>
  <td width="33%" valign="top" align="center"><a href="https://dexfunc.github.io/">
  <video autoplay loop muted src="https://dexfunc.github.io/resources/dexfunc-website-video.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" width="90%"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://dexfunc.github.io/" id="DEXFUNC">
    <heading>Dexterous Functional Grasping</heading></a><br>
    Ananye Agarwal, Shagun Uppal, Kenneth Shaw, Deepak Pathak<br>
    CoRL 2023
    </p>

    <div class="paper" id="dexfunc">
    <a href="https://dexfunc.github.io/">webpage</a> |
    <a href="javascript:toggleblock('dexfunc-abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('dexfunc')" class="togglebib">bibtex</a> |
    <a href="https://arxiv.org/abs/2312.02975">arXiv</a> 

    <p align="justify"> <i style="display: none;" id="dexfunc-abs">While there have been significant strides in dexterous manipulation, most of it is limited to benchmark tasks like in-hand reorientation which are of limited utility in the real world. The main benefit of dexterous hands over two-fingered ones is their ability to pickup tools and other objects (including thin ones) and grasp them firmly to apply force. However, this task requires both a complex understanding of functional affordances as well as precise low-level control. While prior work obtains affordances from human data this approach doesn't scale to low-level control. Similarly, simulation training cannot give the robot an understanding of real-world semantics. In this paper, we aim to combine the best of both worlds to accomplish functional grasping for in-the-wild objects. We use a modular approach. First, affordances are obtained by matching corresponding regions of different objects and then a low-level policy trained in sim is run to grasp it. We propose a novel application of eigengrasps to reduce the search space of RL using a small amount of human data and find that it leads to more stable and physically realistic motion. We find that eigengrasp action space beats baselines in simulation and outperforms hardcoded grasping in real and matches or outperforms a trained human teleoperator.</i></p>

<pre xml:space="preserve" style="display:none;">
@inproceedings{agarwal2023dexterous,
  title={Dexterous Functional Grasping},
  author={Agarwal, Ananye and Uppal, Shagun and Shaw, Kenneth and Pathak, Deepak},
  booktitle={Conference on Robot Learning},
  pages={3453--3467},
  year={2023},
  organization={PMLR}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <tr>
    <td width="33%" valign="top" align="center"><a href="https://dash-through-interaction.github.io/">
    <video autoplay loop muted src="https://dash-through-interaction.github.io/resources/task_videos/Grape.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black"></video>
    </a></td>
    <td width="67%" valign="top">
      <p><a href="https://dash-through-interaction.github.io/" id="DASH">
      <heading>DASH: A Framework for Designing Anthropomorphic Soft Hands through Interaction</heading></a><br>
      Pragna Mannam*, Kenneth Shaw*, Dominik Bauer, Jean Oh, Deepak Pathak, Nancy Pollard<br>
      IEEE Humanoids 2023 <b>Best Oral Paper Award Finalist</b>
      </p>
  
      <div class="paper" id="dash">
      <a href="https://dash-through-interaction.github.io/">webpage</a> |
      <a href="javascript:toggleblock('dash_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('dash')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2306.04784">arXiv</a>
  
      <p align="justify"> <i style="display: none;" id="dash_abs">Modeling and simulating soft robot hands can aid in design iteration for complex and high degree-of-freedom (DoF) morphologies. This can be further supplemented by iterating on the design based on its performance in real world manipulation tasks. However, iterating in the real world requires a framework that allows us to test new designs quickly at low costs. In this paper, we present a framework that leverages rapid prototyping of the hand using 3D-printing, and utilizes teleoperation to evaluate the hand in real world manipulation tasks. Using this framework, we design a 3D-printed 16-DoF dexterous anthropomorphic soft hand (DASH) and iteratively improve its design over five iterations. Rapid prototyping techniques such as 3D-printing allow us to directly evaluate the fabricated hand without modeling it in simulation. We show that the design improves over five design iterations through evaluating the hand’s performance in 30 real-world teleoperated manipulation tasks. Testing over 900 demonstrations shows that our final version of DASH can solve 19 of the 30 tasks compared to Allegro, a popular rigid hand in the market, which can only solve 7 tasks. We open-source our CAD models as well as the teleoperated dataset for further study.</i></p>
  
  <pre xml:space="preserve" style="display:none;">
  @article{mannam2023Dashhand,
  title={DASH: A Framework for Designing Anthropomorphic Soft Hands through Interaction},
  author={Mannam, Pragna* and Shaw, Kenneth* and Bauer, Dominik and Oh, Jean and Pathak, Deepak and Pollard, Nancy},
  journal= {IEEE Humanoids},
  year={2023}
  }
  </pre>
      </div>
    </td>
</tr>


<tr>
  <tr bgcolor="#ffffd0">
    <td width="33%" valign="top" align="center"><a href="https://leap-hand.github.io/">
    <video autoplay loop muted src="images/leap_sim2real.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black"></video>
    </a></td>
    <td width="67%" valign="top">
      <p><a href="https://leap-hand.github.io/" id="LEAP">
      <heading>LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning</heading></a><br>
      Kenneth Shaw, Ananye Agarwal, Deepak Pathak <br>
      RSS 2023
      <br>
      <br>
      <b>Start your dexterous manipulation journey here!</b>
      </p>
  
      <div class="paper" id="leap">
      <a href="https://leap-hand.github.io/">webpage</a> |
      <a href="javascript:toggleblock('leap_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('leap')" class="togglebib">bibtex</a> |
      <a href="https://rss2023.github.io/rss2023-website/program/papers/089/">RSS</a>
  
      <p align="justify"> <i style="display: none;" id="leap_abs">Dexterous manipulation has been a long-standing challenge in robotics. While machine learning techniques have shown some promise, results have largely been currently limited to simulation. This can be mostly attributed to the lack of suitable hardware. In this paper, we present LEAP Hand, a low-cost dexterous and anthropomorphic hand for machine learning research. In contrast to previous hands, LEAP Hand has a novel kinematic structure that allows maximal dexterity regardless of finger pose. LEAP Hand is low-cost and can be assembled in 4 hours at a cost of 2000 USD from readily available parts. It is capable of consistently exerting large torques over long durations of time. We show that LEAP Hand can be used to perform several manipulation tasks in the real worldâ€”from visual teleoperation to learning from passive video data and sim2real. LEAP Hand significantly outperforms its closest competitor Allegro Hand in all our experiments while being 1/8th of the cost. We release the URDF model, 3D CAD files, tuned simulation environment, and a development platform with useful APIs on our website.</i></p>
  
  <pre xml:space="preserve" style="display:none;">
  @article{shaw2023Leaphand,
  title={LEAP Hand:Low-Cost, Efficient,
  and Anthropomorphic Hand for Robot Learning},
  author={Shaw, Kenneth and Agarwal, Ananye
  and, Pathak, Deepak},
  journal= {RSS},
  year={2023}
  }
  </pre>
      </div>
    </td>
</tr>

<tr>
  <td width="33%" valign="top" align="center"><a href="https://video-dex.github.io">
  <video autoplay loop muted src="https://video-dex.github.io/resources/internet.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black"></video>
  </a></td>
  <td width="67%" valign="top">
    <heading>Learning Dexterity from Human Hand Motion in Internet Videos</heading></a><br>
    Kenneth Shaw*, Shikhar Bahl*, Aravind Sivakumar, Aditya Kannan, Deepak Pathak<br>
    IJRR 2022 Special Issue
    </p>
    <div class="paper" id="ijrr_human">
    <a href="javascript:toggleblock('ijrr_human_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('ijrr_human')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="ijrr_human_abs">To build general robotic agents that can operate in many environments, it is often useful for robots to collect experience in the real world.  However, unguided experience collection is often not feasible due to safety, time, and hardware restrictions.  We thus propose leveraging the next best thing as real world experience: videos of humans using their hands. To utilize these videos, we develop a method that retargets any 1st person or 3rd person video of human hands and arms into the robot hand and arm trajectories.   While retargeting is a difficult problem, our key insight is to rely on only internet human hand video to train it.  We use this method to present results in two areas:  First, we build a system that enables any human to control a robot hand and arm, simply by demonstrating motions with their own hand. The robot observes the human operator via a single RGB camera and imitates their actions in real-time.  This enables the robot to collect real-world experience safely using supervision.  Second, we retarget in-the-wild human internet video into task-conditioned pseudo-robot trajectories to use as artificial robot experience.  This learning algorithm leverages action priors from human hand actions, visual features from the images, and physical priors from dynamical systems to pretrain typical human behavior for a particular robot task.  We show that by leveraging internet human hand experience, we need fewer robot demonstrations compared to many other methods.</i></p>

<pre xml:space="preserve" style="display:none;">
  @article{shaw_internetvideos,
    title={Learning Dexterity from Human Hand Motion in Internet Videos},
    author={Shaw, Kenneth and Bahl,
    Shikhar and Sivakumar, Aravind and Kannan, Aditya and Pathak, Deepak},
    journal= {IJRR},
    year={2022}
  }
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://video-dex.github.io">
  <video autoplay loop muted src="images/videodex.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://video-dex.github.io" id="VIDEODEX">
    <heading>VideoDex: Learning Dexterity from Internet Videos</heading></a><br>
    Kenneth Shaw*, Shikhar Bahl*, Deepak Pathak<br>
    CoRL 2022
    </p>

    <div class="paper" id="videodex">
    <a href="https://video-dex.github.io">webpage</a> |
    <a href="javascript:toggleblock('videodex_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('videodex')" class="togglebib">bibtex</a> |
    <a href="https://arxiv.org/abs/2212.04498">arXiv</a> |
    <a href="https://video-dex.github.io">demo</a>

    <p align="justify"> <i style="display: none;" id="videodex_abs">To build general robotic agents that can operate in many environments, it is often imperative for the robot to collect experience in the real world.  However, this is often not feasible due to safety, time and hardware restrictions.  We thus propose leveraging the next best thing as real world experience: internet videos of humans using their hands.  Visual priors, such as visual features, are often learned from videos, but we believe that more information from videos can be utilized as a stronger prior.  We build a learning algorithm, VideoDex, that leverages visual, action and physical priors from human video datasets to guide robot behavior.  These action and physical priors in the neural network dictate the typical human behavior for a particular robot task.   We test our approach on a robot arm and dexterous hand based system and show strong results on many different manipulation tasks, outperforming various state-of-the-art methods.</i></p>

<pre xml:space="preserve" style="display:none;">
  @article{shaw_videodex,
    title={VideoDex: Learning Dexterity
    from Internet Videos},
    author={Shaw, Kenneth and Bahl,
    Shikhar and Pathak, Deepak},
    journal= {CoRL},
    year={2022}
  }
</pre>
    </div>
  </td>
</tr>

<tr bgcolor="#ffffd0">
  <td width="33%" valign="top" align="center"><a href="https://robotic-telekinesis.github.io/">
  <!-- <video autoplay loop muted src="images/rtk.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black"></video> -->
  <img src="images/rtk.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://robotic-telekinesis.github.io/" id="RTK">
    <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
    <heading>Robotic Telekinesis: Learning a Robotic Hand Imitator by Watching Humans on Youtube</heading></a><br>
    Aravind Sivakumar*, Kenneth Shaw*, Deepak Pathak<br>
    RSS 2022<br/>
    <b>Best Paper Award Finalist in Scaling Robot Learning Workshop</b> <!-- https://sites.google.com/view/rss22-srl/home -->
    </p>

    <div class="paper" id="rtk">
    <a href="https://robotic-telekinesis.github.io/">webpage</a> |
    <a href="javascript:toggleblock('rtk_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('rtk')" class="togglebib">bibtex</a> |
    <a href="https://arxiv.org/abs/2202.10448">arXiv</a> |
    <a href="https://youtu.be/46OxlcJ48Bg">demo</a> |
    <a href="https://www.golosameriki.com/a/robotic-telekinesis-interview-carnegie-mellon-research/6531734.html">in the media</a>

    <p align="justify"> <i id="rtk_abs">We build a system that enables any human to control a robot hand and arm, simply by demonstrating motions with their own hand. The robot observes the human operator via a single RGB camera and imitates their actions in real-time. Human hands and robot hands differ in shape, size, and joint structure, and performing this translation from a single uncalibrated camera is a highly underconstrained problem. Moreover, the retargeted trajectories must effectively execute tasks on a physical robot, which requires them to be temporally smooth and free of self-collisions. Our key insight is that while paired human-robot correspondence data is expensive to collect, the internet contains a massive corpus of rich and diverse human hand videos. We leverage this data to train a system that understands human hands and retargets a human video stream into a robot hand-arm trajectory that is smooth, swift, safe, and semantically similar to the guiding demonstration. We demonstrate that it enables previously untrained people to teleoperate a robot on various dexterous manipulation tasks. Our low-cost, glove-free, marker-free remote teleoperation system makes robot teaching more accessible and we hope that it can aid robots that learn to act autonomously in the real world.</i></p>

<pre xml:space="preserve">
@article{telekinesis,
  title={Robotic Telekinesis: Learning a
  Robotic Hand Imitator by Watching Humans
  on Youtube},
  author={Sivakumar, Aravind and
  Shaw, Kenneth and Pathak, Deepak},
  journal={RSS},
  year={2022}
}
</pre>
    </div>
  </td>
</tr>


</table>
<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right"><font size="1.5">
    Modified version of template from <a href="http://www.cs.berkeley.edu/~barron/">here</a> and <a href="https://www.cs.cmu.edu/~dpathak/">here</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('rtk_abs');
</script>
</body>

</html>